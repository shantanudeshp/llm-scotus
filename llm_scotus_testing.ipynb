{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Adcm3Cxuixz2L2jjNhwF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shantanudeshp/llm-scotus/blob/20230904-wip/llm_scotus_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "id": "BzT68eur5cWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import openai\n",
        "from getpass import getpass"
      ],
      "metadata": {
        "id": "p62U2rRw5epC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openai_key = getpass('Enter the openai key')"
      ],
      "metadata": {
        "id": "SGYFSlJ95rrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2lRiej2vA_P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "openai.organization = \"org-YjWOHW4W2uUvobjJfFBOBkU8\"\n",
        "openai.api_key = openai_key  # os.getenv(\"OPENAI_API_KEY\")\n",
        "openai.Model.list()"
      ],
      "metadata": {
        "id": "ta0AQt3VvF4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "instructions = '''The following is a brief outline of a case. Predict the verdict of the Supreme Court by responding with the name of the Contestant they would most likely side with.\n",
        "For example:\n",
        "If the contestants are Reagan v. Alabama, respond with either Reagan or Alabama.\n",
        "If the contestants are Lubkowitz v. Smith, respond with either Lubkowitz or Smith.\n",
        "If the contestants are Department of Defense v. Higgins, respond with either Department of Defense or Higgins.'''"
      ],
      "metadata": {
        "id": "dgin_8v0vjne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template1 = \"\"\"\n",
        "{instructions}\n",
        "\n",
        "Case: {case_name}\n",
        "\n",
        "Summary: {case_summary}\n",
        "\n",
        "Verdict:\n",
        "\"\"\"\n",
        "\n",
        "print(template1)"
      ],
      "metadata": {
        "id": "ge12a9S3vGYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "def scrape_contestants(url):\n",
        "    # Get the webpage content\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an error if the request was unsuccessful\n",
        "\n",
        "    # Parse the webpage using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the <h1> tag by its specific attributes\n",
        "    h1_tag = soup.find('h1', attrs={\"id\": \"text-a\", \"class\": \"heading-1 has-margin-top-10 has-margin-bottom-10\"})\n",
        "\n",
        "    if not h1_tag:\n",
        "        return \"Case title not found\"\n",
        "\n",
        "    # Use regular expression to match the pattern \"Word v. Word\"\n",
        "    match = re.search(r'([\\w\\s]+)\\sv\\.\\s([\\w\\s]+)', h1_tag.text)\n",
        "    if match:\n",
        "        return match.group(1).strip() + \" v. \" + match.group(2).strip()\n",
        "    else:\n",
        "        return \"Pattern not found in title!\""
      ],
      "metadata": {
        "id": "IK70Ww7fvOnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scrape_syllabus(url):\n",
        "    # Get the webpage content\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an error if the request was unsuccessful\n",
        "\n",
        "    # Parse the webpage using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find the syllabus section by div id (assuming id=\"diminished-text-3\" based on the provided example)\n",
        "    syllabus_div = soup.find('div', {'id': 'diminished-text-3'})\n",
        "\n",
        "    if not syllabus_div:\n",
        "        return \"Syllabus not found\"\n",
        "\n",
        "    # Find the <em>Held</em> tag\n",
        "    held_tag = syllabus_div.find('em', string=lambda x: x and 'Held' in x)\n",
        "\n",
        "    if held_tag:\n",
        "        # Create an empty list to store paragraphs before the \"Held\" tag\n",
        "        paragraphs_before_held = []\n",
        "\n",
        "        # Iterate over previous siblings of the \"Held\" tag to get content before it\n",
        "        for sib in held_tag.find_all_previous():\n",
        "            if sib.name == 'p':\n",
        "                paragraphs_before_held.insert(0, sib.get_text(strip=True))  # Prepend to keep order\n",
        "            elif sib == held_tag:  # Stop once we reach the 'Held' tag itself\n",
        "                break\n",
        "\n",
        "        syllabus_content = \" \".join(paragraphs_before_held[:-1])  # Excluding the last paragraph which contains 'Held:'\n",
        "    else:\n",
        "        # If no held tag, extract entire syllabus\n",
        "        syllabus_content = syllabus_div.get_text(strip=True)\n",
        "\n",
        "    return syllabus_content"
      ],
      "metadata": {
        "id": "ZsWMoEKgvO7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Assuming you've already defined your scrape_syllabus and scrape_contestants functions...\n",
        "def generate(prompt, engine=\"text-davinci-003\", max_tokens=100):\n",
        "    response = openai.Completion.create(\n",
        "        engine=engine,\n",
        "        prompt=prompt,\n",
        "        max_tokens=max_tokens\n",
        "    )\n",
        "    tokens_used = response.usage['total_tokens']\n",
        "    return response.choices[0].text.strip(), tokens_used\n",
        "\n",
        "def get_verdict(urls):\n",
        "    results = []\n",
        "\n",
        "    for url in urls:\n",
        "        # 1. Call the scrape_contestants and scrape_syllabus functions on each URL\n",
        "        case_name = scrape_contestants(url)\n",
        "        case_summary = scrape_syllabus(url)\n",
        "\n",
        "        # 2. Put the outputs inside the template\n",
        "        prompt = template1.format(instructions=instructions, case_name=case_name, case_summary=case_summary)\n",
        "\n",
        "        # 3. Call the LLM API to predict the verdict for each\n",
        "        predicted_verdict, tokens_used = generate(prompt)\n",
        "\n",
        "        # 4. Store the verdict and tokens used for each URL\n",
        "        results.append({\n",
        "            \"url\": url,\n",
        "            \"predicted_verdict\": predicted_verdict,\n",
        "            \"tokens_used\": tokens_used\n",
        "        })\n",
        "\n",
        "    # 5. Return the results\n",
        "    return results\n",
        "\n",
        "# Example usage:\n",
        "urls = ['https://supreme.justia.com/cases/federal/us/597/21-954/']  # Add more URLs as needed\n",
        "verdicts = get_verdict(urls)\n",
        "\n",
        "for verdict in verdicts:\n",
        "    print(f\"URL: {verdict['url']}, Predicted Verdict: {verdict['predicted_verdict']}, Tokens used: {verdict['tokens_used']}\")"
      ],
      "metadata": {
        "id": "PJ6XslApvPMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def get_case_links(base_url):\n",
        "    response = requests.get(base_url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Find all the links to individual cases based on the structure you provided\n",
        "    links = [div.a['href'] for div in soup.select('.has-padding-content-block-30.-zb.search-result')]\n",
        "\n",
        "    return links\n",
        "\n",
        "def main():\n",
        "    base_url = 'https://supreme.justia.com/cases/federal/us/year/2022.html'\n",
        "\n",
        "    # Get all individual case links\n",
        "    case_links = get_case_links(base_url)\n",
        "\n",
        "    # Convert relative links to full URLs\n",
        "    full_urls = ['https://supreme.justia.com' + link for link in case_links]\n",
        "\n",
        "    # Call your scraping function with the list of URLs\n",
        "    get_verdict(full_urls)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2qNwEAvavkql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def extract_case_urls(years):\n",
        "    base_url = 'https://supreme.justia.com/cases/federal/us/year/{}.html'\n",
        "\n",
        "    # Updated regex pattern to match specific case URLs\n",
        "    pattern = re.compile(r\"/cases/federal/us/\\d+/\\d+-\\d+/\")\n",
        "\n",
        "    all_case_urls = []\n",
        "\n",
        "    for year in years:\n",
        "        url = base_url.format(year)\n",
        "        response = requests.get(url)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        for a in soup.find_all('a', href=True):\n",
        "            if pattern.match(a['href']):\n",
        "                full_url = \"https://supreme.justia.com\" + a['href']\n",
        "                all_case_urls.append(full_url)\n",
        "\n",
        "    return all_case_urls\n",
        "\n",
        "years_list = [2022, 2023, 2020]\n",
        "all_case_urls = extract_case_urls(years_list)\n",
        "\n",
        "# Now, `all_case_urls` contains the case URLs for the provided years.\n",
        "for url in all_case_urls:\n",
        "    print(url)\n",
        "#print(all_case_urls)"
      ],
      "metadata": {
        "id": "9JXVdTuO7iQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "case_urls"
      ],
      "metadata": {
        "id": "XE4BtI3fOx1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_verdict"
      ],
      "metadata": {
        "id": "wS_xjqipRvc8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}